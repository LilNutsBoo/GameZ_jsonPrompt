import streamlit as st
import json
from pathlib import Path
from util import normalize_id_list
from data_loader import load_local_json, fetch_airtable_table
from util import get_connected_field_map
from export_module_optimization import format_tasks, format_npcs, format_reference_data, format_world_timeline,  format_events,  format_items
import zipfile
from io import BytesIO


# ========== ğŸ§­ å­—æ®µæ˜ å°„é…ç½®ï¼ˆé›†ä¸­ç®¡ç†ï¼Œä¾¿äºç»´æŠ¤ Airtable å­—æ®µæ›´åï¼‰ ==========
FIELD_MAP = get_connected_field_map()

# ========== ğŸ§¹ å·¥å…·å‡½æ•°ï¼šå‰”é™¤ç©ºå­—æ®µ ==========
def clean_fields(data):
    if isinstance(data, dict):
        return {k: clean_fields(v) for k, v in data.items() if v not in (None, [], {}, "")}
    elif isinstance(data, list):
        return [clean_fields(i) for i in data if i not in (None, [], {}, "")]
    else:
        return data

# ========== ğŸ“¦ äºŒæ¬¡æ•°æ®æ‹‰å–ï¼Œä»å·²ç»é€‰å‡ºçš„npcå’Œä»»åŠ¡ä¸­ï¼Œæ‹‰å–æ‰€æœ‰ç›¸å…³çš„æ•°æ® ==========
def render_export_panel(world_timeline, selected_tasks, autonomous_npcs, today):
    if not (isinstance(selected_tasks, list) and isinstance(autonomous_npcs, list) and today):
        st.warning("âš ï¸ ç¼ºå°‘å¿…è¦æ•°æ®ï¼Œè¯·ç¡®ä¿ä»»åŠ¡ç­›é€‰å™¨å’Œ NPC ç­›é€‰å™¨å‡å·²è¿è¡Œå®Œæ¯•")
        return

    st.markdown(f"ğŸ“… å½“å‰æ—¥æœŸï¼š{today}")

    task_ids, npc_ids, event_ids, item_ids = set(), set(), set(), set()

    for task in selected_tasks:
        f = task.get("fields", {})
        task_ids.add(f.get(FIELD_MAP["task_id"]))
        task_ids.update(normalize_id_list(f.get(FIELD_MAP["child_tasks"], [])))
        task_ids.update(normalize_id_list(f.get(FIELD_MAP["parent_tasks"], [])))
        npc_ids.update(normalize_id_list(f.get(FIELD_MAP["main_npcs"], [])))
        npc_ids.update(normalize_id_list(f.get(FIELD_MAP["secondary_npcs"], [])))
        event_ids.update(normalize_id_list(f.get(FIELD_MAP["linked_events"], [])))
        event_ids.update(normalize_id_list(f.get(FIELD_MAP["event_codes"], [])))
        item_ids.update(normalize_id_list(f.get(FIELD_MAP["involved_items"], [])))

    for npc in autonomous_npcs:
        fields = npc.get("fields", {})
        npc_ids.add(npc.get("npc_id"))
        task_ids.update(normalize_id_list(fields.get(FIELD_MAP["triggered_tasks"], [])))
        task_ids.update(normalize_id_list(fields.get(FIELD_MAP["related_tasks"], [])))
        event_ids.update(normalize_id_list(fields.get(FIELD_MAP["interaction_logs"], [])))

    id_buckets = {
        "ä»»åŠ¡": list(task_ids),
        "NPC": list(npc_ids),
        "äº‹ä»¶": list(event_ids),
        "ä»»åŠ¡ç‰©å“": list(item_ids),
    }

    source_path = st.session_state.get("raw_data_source")
    use_api = st.session_state.get("independent_api_access", False)

    selected_and_connected = {"ä»»åŠ¡": [], "NPC": [], "äº‹ä»¶": [], "ä»»åŠ¡ç‰©å“": []}

    for table in ["ä»»åŠ¡", "NPC", "äº‹ä»¶", "ä»»åŠ¡ç‰©å“"]:
        ids_to_match = set(id_buckets.get(table, []))
        results = []

        if source_path and not use_api:
            full_data = load_local_json(source_path, f"{table}.json")
        else:
            try:
                full_data = fetch_airtable_table(table)
            except Exception as e:
                st.error(f"âŒ æ‹‰å–è¡¨ {table} å¤±è´¥ï¼š{e}")
                full_data = []

        for record in full_data:
            rid = record.get("id") or record.get("record_id")
            if rid in ids_to_match:
                cleaned = clean_fields(record)
                results.append(cleaned)

        selected_and_connected[table] = results

    if st.session_state.get("show_debug"):
        with st.expander("ğŸ§ª selected_and_connected é¢„è§ˆ", expanded=False):
            for t, data in selected_and_connected.items():
                st.markdown(f"**{t}ï¼š{len(data)} æ¡è®°å½•**")
                st.write(data)

# ====================================== ä¼˜åŒ–å¹¶å¯¼å‡º ========================================
    # å¯¼å‡º json
    optimized_selected_tasks = format_tasks(selected_tasks, selected_and_connected)
    optimized_autonomous_npcs = format_npcs(autonomous_npcs, selected_and_connected)
    optimized_selected_and_connected = format_reference_data(selected_and_connected) 
    optimized_world_timeline = format_world_timeline(world_timeline)

    if st.session_state.get("show_debug"):
        with st.expander("ğŸ§ª selected_tasksé¢„è§ˆ:", expanded=False):
            st.write(selected_tasks) 
        with st.expander("ğŸ§ª ä¼˜åŒ–äº†çš„selected_tasksé¢„è§ˆ:", expanded=False):
            st.write(optimized_selected_tasks) 
        with st.expander("ğŸ§ª ä¼˜åŒ–äº†çš„autonomous_npcsé¢„è§ˆ:", expanded=False):
            st.write(optimized_autonomous_npcs)
        with st.expander("ğŸ§ª ä¼˜åŒ–äº†çš„reference_dataé¢„è§ˆ:", expanded=False):
            st.write(optimized_selected_and_connected)

    today_export = {
        "today": today,
        "world_background" : optimized_world_timeline,
        "tasks_for_today": optimized_selected_tasks,
        "autonomous_npcs_for_today": optimized_autonomous_npcs,
        "reference_data": optimized_selected_and_connected
    }

    st.download_button("ğŸ“¥ ä¸‹è½½ å½“æ—¥å‰§æƒ…é‡ç‚¹", data=json.dumps(today_export, ensure_ascii=False, indent=2), file_name=f"å½“æ—¥å‰§æƒ…é‡ç‚¹_{today}.json", mime="application/json")

def create_export_zip(source_path):
    buffer = BytesIO()
    use_api = st.session_state.get("independent_api_access", False)

    # for table in ["ä»»åŠ¡", "NPC", "äº‹ä»¶", "ä»»åŠ¡ç‰©å“"]:

    if source_path and not use_api:
        timeline_data = load_local_json(source_path, f"Timeline.json")
        tasks_data = load_local_json(source_path, f"ä»»åŠ¡.json")
        npcs_data = load_local_json(source_path, f"NPC.json")
        events_data = load_local_json(source_path, f"äº‹ä»¶.json")
        items_data = load_local_json(source_path, f"ä»»åŠ¡ç‰©å“.json")
        # ---------------------------------------------------
        all_refr = {"Timeline": [], "ä»»åŠ¡": [], "NPC": [], "äº‹ä»¶": [], "ä»»åŠ¡ç‰©å“": []}
        for table in ["Timeline", "ä»»åŠ¡", "NPC", "äº‹ä»¶", "ä»»åŠ¡ç‰©å“"]:
            results = []
            if source_path and not use_api:
                full_data = load_local_json(source_path, f"{table}.json")
            else:
                try:
                    full_data = fetch_airtable_table(table)
                except Exception as e:
                    st.error(f"âŒ æ‹‰å–è¡¨ {table} å¤±è´¥ï¼š{e}")
                    full_data = []
            for record in full_data:
                cleaned = clean_fields(record)
                results.append(cleaned)
            all_refr[table] = results
        # =------------------------------------------
        timeline_data = format_world_timeline(timeline_data)
        tasks_data = format_tasks(tasks_data, all_refr)
        npcs_data = format_npcs(npcs_data, all_refr)
        events_data = format_events(events_data)
        items_data = format_items(items_data)

    with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED) as zipf:
        zipf.writestr(f"complete_timeline.json", json.dumps(timeline_data, ensure_ascii=False, indent=2))
        zipf.writestr(f"all_tasks.json", json.dumps(tasks_data, ensure_ascii=False, indent=2))
        zipf.writestr(f"all_NPCs.json", json.dumps(npcs_data, ensure_ascii=False, indent=2))
        zipf.writestr(f"all_events.json", json.dumps(events_data, ensure_ascii=False, indent=2))
        zipf.writestr(f"all_items.json", json.dumps(items_data, ensure_ascii=False, indent=2))

    buffer.seek(0)
    return buffer
